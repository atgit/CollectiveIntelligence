# Concepts
- Bit - distinction between two states
- Entropy is Information You Don't Have - log2W
- Code - systematic way to represent information
- Redundancy
  - Advantages: deal with noise; error correction
  - Compression
  - Error detection and correction
- Probability
  - n->âˆž, occurrence of event/n converges to P  (n = times of trials)
  - Features
    - Exhaustive
    - Mutually exclusive
  - Prior probability
  - event - all the possible outcome
- Noise
  - Binary symetric channel

- Maximum entropy
  - Information theory provides a constructive criterion for setting up probability distributions on the basis of partial knowledge, and leads to a type of statistical inference which is called the maximum entropy estimate. It is least biased estimate possible on the given information; i.e., it is maximally noncommittal with regard to missing information.